{"index": 1 , "query": "hi, i cant find snowflake in the source types", "response": "At present, Hevo does not support Snowflake as a source; it is only available as a destination. \n\nFor a complete list of supported sources and destinations, please refer to the documents below.\n\n1. Supported Sources[https://hevodata.com/integrations/pipeline/]\n2. Supported Destination[https://hevodata.com/integrations/pipeline/?is_destination=true]"}
{"index": 2 , "query": "I want help in setting up first pipeline, shopify and bigquery", "response": "Follow the steps below to set up Shopify as your source and BigQuery as your destination. \n\nRefer to: \n\n1. Shopify as a Source[https://docs.hevodata.com/sources/prod-analytics/shopify/] \n2. BigQuery as a Destination[https://docs.hevodata.com/destinations/data-warehouses/google-bigquery/]"}
{"index": 3 , "query": "Can you confirm that hevo is able to load data from my sources to the SQL database of Microsoft Fabric. https://learn.microsoft.com/en-us/fabric/database/sql/connect", "response": "Currently, Microsoft Fabric is not supported as a destination in Hevo. However, we do support SQL Server as a destination outside of Microsoft Fabric.\n\nYou can explore the destinations supported by Hevo in the following document. \n\nRefer to: \nSupported Destinations[https://hevodata.com/integrations/pipeline/?is_destination=true]"}
{"index": 4 , "query": "Hello team, does Hevo provide capability of pushing data to MessageBird via API?\nI'm referring to this system- https://developers.messagebird.com/api/#messagebird-api-reference\nWe have a dataset in Bigquery. We want t push these data to MessageBird's database. We're looking for platform that can handle this flow.", "response": "Hevo supports BigQuery as a source, enabling seamless data ingestion. However, MessageBird is not supported as a destination at the moment, so pushing data to MessageBird is not possible as of now. \n\nFor a complete list of supported sources and destinations, refer to the documents below: \n\n1. Supported Sources[https://hevodata.com/integrations/pipeline/] \n2. Supported Destinations[https://hevodata.com/integrations/pipeline/?is_destination=true]"}
{"index": 5 , "query": "We are planning to ingest data from various marketing platforms and before we do, we would like to understand the pricing around these data soruces\nFacebook/Meta\nSnapchat\nTiktok\nfor now", "response": "Hevo offers free sources that can be used without any paid plan. However, for using paid sources, you will need to be on a PAID plan. Our Starter Plan starts at $299/month or $239/month (billed yearly). You can also contact our sales team for a curated plan based on your requirements. \n\nRefer:\nSupported Sources[https://hevodata.com/integrations/pipeline/] \n\nHevo does not charge any additional fees for using a source; you will only be charged for the events you load to your destination from these sources, based on their availability in your plan. \n\nTo create pipelines from these sources, refer to the documentation below: \n\nFacebook Ads[https://docs.hevodata.com/sources/mkt-analytics/facebook-ads/] \nSnapchat Ads [https://docs.hevodata.com/sources/mkt-analytics/snapchat-ads/] \nTikTok Ads[https://docs.hevodata.com/sources/mkt-analytics/tiktok-ads/]"}
{"index": 6 , "query": "How can I rename a destination table while ensuring Hevo still recognizes it?", "response": "You can rename your destination table after creating the pipeline. However, renaming it will generate a new destination table where only incremental data will be loaded. To load all data into the new table, you must restart the historical load for the event type.\n\nIf you need to change the destination table name, follow the steps in the document below:\n\nChange Destination Table Name[https://docs.hevodata.com/destinations/destination-faqs/change-destination-table-name/]\n\nAs per product behaviour, once a mapping is established with an existing table name, the name cannot be modified. Changing the destination table name will cause the pipeline to stop detecting the original table, resulting in the creation of a new table with the updated name. Consequently, restarting the historical load will be necessary."}
{"index": 7 , "query": "Hello team,\ni want to migrate my database from mysql to postgresql can you please suggest me some plans and feature for it on your portal\nif all process is smooth and good i wll suggest client to purchase your plan", "response": "You can migrate data from a MySQL database to a PostgreSQL database by setting up a pipeline. Simply configure MySQL as the source and PostgreSQL as the destination. Please refer to the documents below for guidance: \n\n1. MySQL as a Source[https://docs.hevodata.com/sources/dbfs/databases/mysql/] \n2. PostgreSQL as a Destination[https://docs.hevodata.com/destinations/databases/postgresql/] \n\nIf you’d like to schedule a demo to understand our product better, you can do so by filling out the form below: \n\nSchedule Demo[https://hevodata.com/schedule-demo/?step=email&set=2]"}
{"index": 8 , "query": "Hi,\ncan you please tell me your plans details?\nThanks", "response": "Hevo offers 150+ native connectors to seamlessly move your data.\nLearn more here[https://hevodata.com/integrations/pipeline/] \n\nWe provide several plans to suit your needs: \n\n1. Free Plan: Supports up to 1M events per month and includes all source connectors under the free tier. \n\n2. Starter Plan: Starts at $299/month (billed monthly) or $239/month (billed yearly) for 5M events per month, scaling up to 50M events per month for $949/month (billed monthly) or $759/month (billed yearly). Includes 10 users, dbt integration, SSH/SSL, and 24x5 support. \n\n3. Professional Plan: Starts at $849/month (billed monthly) or $679/month (billed yearly) for 20M events per month, scaling up to 100M events per month for $2199/month (billed monthly) or $1759/month (billed yearly). Includes everything in the Starter Plan, plus unlimited users, streaming pipelines, REST APIs, and Reverse SSH. \n\n4. Business Critical Plan: Includes everything in the Professional Plan, plus role-based access control, single sign-on, multiple workspaces, VPC peering, and advanced security certificates. \n\nAdd-ons are available for Professional and Business Critical plans. \n\nFor more details, refer here[https://hevodata.com/pricing/pipeline/]"}
{"index": 9 , "query": "actually we are running out of credits for this month. I would like to know how this on demand credit works", "response": "On-Demand Credit in Hevo allows you to maintain a credit balance that can be used to purchase events, ensuring that your pipelines continue running even after your events quota is exhausted.\n\nHere’s how it works:\n\nCredit Utilization: When your base plan’s events are consumed, Hevo will utilize the events in your On-Demand Credit to load data to the destination, preventing any disruption in your pipelines.\n\nSetting the Limit: You can set and update your On-Demand Credit limit at any time during the current monthly usage cycle. The limit can be set up to a maximum of 60% of your subscribed plan’s events quota.\n\nPayment and Redemption: You can pay for your consumed credits at any time during the month, which resets your credit utilization to zero, enabling you to continue using on-demand credits again without interruption.\n\nViewing Usage: You can monitor your current and past six months’ credit usage in the On-Demand Credit section of the Plan Details tab.\n\nFor more details, you can refer to the following document:\nOn-demand credits[https://docs.hevodata.com/account-management/billing/on-demand-purchases/on-demand-credit/]"}
{"index": 10 , "query": "Hello, for BinLog CDC, if I change my ingestion frequency to be more frequent, will it change anything in terms of costs for me on Hevo?", "response": "For BinLog pipelines, we read and replicate all changes recorded in the logs. Therefore, adjusting the ingestion frequency will not impact your costs in Hevo. \n\nHevo only charges you for the number of events you load to your destination."}
{"index": 11 , "query": "Is MongoDB supported ?\nOur Source and Destination are MongoDB", "response": "MongoDB as a source is supported by Hevo in both Generic and Atlas variations. However, as of now, MongoDB as a destination is not supported. \n\nYou can find the complete list of sources and destinations supported by Hevo in the following links: \n\n1. Sources[https://hevodata.com/integrations/pipeline/] \n2. Destinations[https://hevodata.com/integrations/pipeline/?is_destination=true]"}
{"index": 12 , "query": "my company's website has hubspot chatbot integrated to it.\nwant to know if it is possible to get that chat data to snowflake via hevodata or not", "response": "We will not be able to support data replication from the chatbot. The replicated data is governed by HubSpot's ERD and is limited to responses from the HubSpot API. \n\nHevo follows the following ERD for HubSpot[https://docs.hevodata.com/sources/mkt-analytics/hubspot/#schema-and-primary-keys]"}
{"index": 13 , "query": "is Hevo have Oracle as source and Destination", "response": "As of now, we do have Oracle as a source, but it is not available as a destination. You can refer to the document below for the list of sources and destinations that are available in Hevo.\n\nRefer: \nSupported Sources[https://hevodata.com/integrations/pipeline/] \nDestinations[https://hevodata.com/integrations/pipeline/?is_destination=true]"}
{"index": 14 , "query": "Hello, I’m wondering for the transformation is it etl or elt and how can I make sure it’s etl?\nit’s important for the privacy of my data", "response": "Hevo is offered as a public SaaS product hosted on AWS. Hevo's Pipelines retrieve data from your data source, perform in-flight transformations, and load it to your destination. \n\nHevo Data Inc. is committed to ensuring the privacy and confidentiality of all the user data processed by our systems and applications. \n\nWe are SOC2, HIPAA, GDPR, and CCPA compliant. \nRegulatory Compliance[https://docs.hevodata.com/introduction/regulatory-compliance/] \n\nWhatever data passes through our systems is encrypted with the AES algorithm and is permanently deleted as soon as it is loaded to your destination.\nRefer:\nData Retention and Encryption[https://docs.hevodata.com/introduction/security/customer-data-retention-and-encryption/]"}
{"index": 15 , "query": "I am trying to connect Tableau REST API as a Source on a Pipeline. I was able to get all the connection setup but Tableau REST API data only response is in CSV format. I think what you need is in JSON format. Is there a way you can receive CSV?", "response": "As of now, Hevo supports only JSON content types in API responses, so it is not possible for us to fetch CSV data. \n\nRefer: \nREST API Limitations[https://docs.hevodata.com/sources/engg-analytics/streaming/rest-api/#limitations]"}
{"index": 16 , "query": "Hi, I have a google sheet pipeline and I'd like to be able to detele the row in the google sheet and it stay in the database. Previously I thought the 'hevo_marked_deleted' would populate in this instance but it hasnt", "response": "The hevo_marked_deleted column is intended to track deletions in supported sources without performing hard deletes. Instead, we set hevo_marked_deleted = true when records are marked for deletions. \n\nWhile Google Sheets doesn't currently support tracking deleted records, this column is included to maintain consistency across destinations. If you are working with other sources that support deletion tracking, this column will indicate records marked as deleted. In Google Sheets, however, rows removed from the sheet are not captured as deletions, and the column will remain unused for this purpose. \n\nRefer: \nHevo generated metadata[https://docs.hevodata.com/data-ingestion/hevo-generated-metadata/]"}
{"index": 17 , "query": "I have a question on transformations: can a transformation do a call out to an API?\nI would like to be able to call an external API from the transformation and pass some of the event properties in order to trigger some external workflow", "response": "As of now, it is not possible to call an API from the transformation section of a pipeline. \n\nThe Transformation is useful when the data from the source system may sometimes be unsuitable for direct replication to the destination database or data warehouse. With Hevo's transformations, you can modify and prepare the data in various ways before loading it into the destination. \n\nWe do have API endpoints for you to automate pipelines, models, and other related tasks in Hevo.\n\nRefer: \nHevo API[https://api-docs.hevodata.com/]"}
{"index": 18 , "query": "Hi on the free version can I ingest from a rest api and output to mysql", "response": "You will need to take a paid plan to use the REST API source. \n\nCurrently, we have the following list of sources available under the free plan. \nRefer:\nSupported Sources[https://hevodata.com/integrations/pipeline/?is_free=true]"}
{"index": 19 , "query": "Do hevo charge for Initial or any Historical Load ?", "response": "Historical Load in Hevo is always free, so restarting it will not incur any event consumption."}
{"index": 20 , "query": "Hi Anmol , I just deleted a pipeline mistakenly can you please revert it back ?\nor is there any way I can do it ?", "response": "It is not possible to revert a deleted pipeline. We do not store any information about the pipeline once it is deleted. \n\nYou will need to create a new pipeline."}
{"index": 21 , "query": "Hello Team. Today I created a new pipeline 48. While ingesting data it says no space left on device. What does it mean?", "response": "Error: Error writing file '/tmp/#sql/fd=483' (Errcode: 28 \"No space left on device\") Root Cause: This error indicates that there isn’t enough space to create the temporary files MySQL requires when querying data from the tables. Resolution: To resolve this issue, you can allocate additional space to the /tmp partition or create an index on the relevant columns to minimize the space needed for query execution."}
{"index": 22 , "query": "HI Jashmitha One of our pipeline is showing Free Trial just wanted to understand why", "response": "Once you subscribe to a plan, all unused Sources are eligible for a free 14-day trial with unlimited Event usage. An unused Source refers to any Source that has not yet been used to create a Pipeline. During the trial period, you can create unlimited Pipelines using one or more of these Sources at no additional cost. This is why it's showing as a free trial.Reference: Free Trial of Unused Sources [https://docs.hevodata.com/introduction/free-trials/free-trial-of-unused-sources/]"}
{"index": 23 , "query": "Pipeline - 134 I am not seeing any option to load historical data found an option at source config to load historical data which was disabled while creating, now I am unable to enable the same. need help regarding this", "response": "If you wish to ingest historical data, you will need to enable the Load Historical Data option during the pipeline creation process. The Load Historical Data option is non-editable once a pipeline has been created. Unfortunately, it cannot be modified for an existing pipeline. To ingest historical data, you will need to create a new pipeline and enable the Load Historical Data option, which is available under Advanced Settings during pipeline creation."}
{"index": 24 , "query": "Hi Jashmitha One of my pipelines (#6) is throwing an error message: \"The token used in the request has expired\"", "response": "Could you please reauthorize the accounts used to create this pipeline? Reference: Source Account Reauthorization [https://docs.hevodata.com/getting-started/connection-options/reauthorizing-oauth-account/#source-account-reauthorization] Once reauthorized, please trigger Run Now."}
{"index": 25 , "query": "Hi. Our destination database was offline for a little bit. \"Failed to execute Auto Mapping instruction due to reason: net.snowflake.client.jdbc.SnowflakeSQLException: SQL compilation error: Schema 'PC_HEVODATA_DB.PUBLIC' does not exist or not authorized.\" Will it replay on its own now that things are back online?", "response": "This is a known issue, and we have a document that outlines all the possible causes and suggestions to resolve the error. I recommend reviewing it, and please let us know if you continue to encounter the same issue. Reference: Schema not found [https://docs.hevodata.com/troubleshooting/destinations/snowflake/errors-post-pipeline-creation/schema-not-found/] Please verify if everything is configured according to the document."}
{"index": 26 , "query": "Hi! I was hoping you could tell me which of the following data sets are the ones that are billable on my detailed usage summary? I have the colums: Total Ingested, Total Loaded, total incremental loaded. I am trying to figure out which column details how much I get charged", "response": "You will be charged for the total incremental load of the data.For more details about event usage, refer to the following document: Reference: [Events-Usage]. https://docs.hevodata.com/events-usage/To understand how you are charged, you can check the billing history in the overview section. Reference: Billing History[https://docs.hevodata.com/account-management/billing/billing-history/]"}
{"index": 27 , "query": "I have created a pipeline using hevo api Include new object is set as true. But I want it as false. But I don't find variable name to set this value in API doc. Can you please help", "response": "You can set the Include New Object value to false using the Hevo API. Please follow these steps: 1. Navigate to the Update Pipeline Source Configurations endpoint: Reference: Udate Source Config [https://api-docs.hevodata.com/reference/updatesourceconfig]. 2. Add the following field to the configuration object: Key: include_new_tablesValue: false"}
{"index": 28 , "query": "Pinter. out me to API which will help me adde event types to schema mapper of pipeline", "response": "To create schema mapping for an existing pipeline, you can utilize the \"Update Schema Mapping\" endpoint (/updateschemamapping).For more details, please refer to the Update Schema Mapping API documentation.[https://api-docs.hevodata.com/reference/updateschemamapping]"}
{"index": 29 , "query": "In pipeline number #57, for the object \"employee,\" duplicate fields have been created for some of the attributes. 1. Could you confirm if these fields were auto-mapped or manually created? 2. If they were manually created, please let me know the process to merge the fields and consolidate all the data into a single field.", "response": "It appears that this object was manually mapped. If you’d like to load all the fields into a single target field, you can use the Schema Mapper feature. Here's how: 1. Navigate to the Schema Mapper page in your pipeline. 2. Locate the duplicate fields for the \"employee\" object. 3. Select the source field you wish to map and align it with the appropriate target field. Reference: Manual-mapping[https://docs.hevodata.com/pipelines/schema-mapper/manually-mapping-et/]"}
{"index": 30 , "query": "Hello, good morning. How see the invoice copy", "response": "To access your invoice copy in Hevo, please follow these steps: 1. Click the drop-down menu next to your username in the User Information Panel. 2. Select Billing. 3. On the Plan Details page, navigate to the left-hand pane and click on Billing History. On the Billing History page, you will find all invoices generated for your account along with their payment statuses. You can download a PDF copy of any invoice by clicking on the corresponding icon, where available. Reference: Billing History[https://docs.hevodata.com/account-management/billing/billing-history/]"}
